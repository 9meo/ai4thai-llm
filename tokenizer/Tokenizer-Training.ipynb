{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0042e0-9ee0-4dd3-8997-bf989116f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1847615-5413-4bd8-b73f-a16fdc6a79d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token  hf_...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d2810cd-c15b-42db-b4cc-eb9a712d49d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from transformers import LlamaTokenizerFast, TrainingArguments, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22e696a1-65d6-409c-bac8-0bb360a1072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "\n",
    "    # Load the dataset from the huggingface Hub and prepare it for training\n",
    "    if args.dataset_name is not None:\n",
    "        dataset = load_dataset(args.dataset_name, \n",
    "            split=args.dataset_split, \n",
    "            token=args.hub_token if args.hub_token else None,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"No dataset name provided or dataset is already tokenized\") \n",
    "\n",
    "    # Remove non text columns\n",
    "    dataset = dataset.remove_columns([col for col in dataset.column_names if col != \"text\"])\n",
    "\n",
    "    # select `num_samples` from the dataset\n",
    "    dataset = dataset.shuffle(seed=42).select(range(args.num_samples))\n",
    "\n",
    "    # Create a SentencePieceBPETokenizer\n",
    "    tokenizer = SentencePieceBPETokenizer()\n",
    "\n",
    "    # Train the SentencePieceBPETokenizer on the dataset\n",
    "    tokenizer.train_from_iterator(\n",
    "        iterator=dataset['text'],\n",
    "        vocab_size=args.vocab_size,\n",
    "        show_progress=True,\n",
    "        special_tokens=[\"<unk>\", \"<s>\", \"</s>\",  \"<pad>\"],\n",
    "    )\n",
    "\n",
    "    # Save the tokenizer\n",
    "    tokenizer.save(\"new-sentencepiece-tokenizer.json\", pretty=True)\n",
    "\n",
    "    # Load reference tokenizer\n",
    "    if args.reference_tokenizer is not None and args.hub_token is not None:\n",
    "        reference_tokenizer = AutoTokenizer.from_pretrained(args.reference_tokenizer)\n",
    "        reference_tokenizer.save_pretrained(\"reference-tokenizer\")\n",
    "    else:\n",
    "        raise ValueError(\"No tokenizer name provided or no hub token provided. Try using `--reference_tokenizer 'meta-llama/Llama-2-7b-hf'\")\n",
    "\n",
    "    # Read and dump the json file for the new tokenizer and the reference tokenizer\n",
    "    with open(\"new-sentencepiece-tokenizer.json\") as f:\n",
    "        new_llama_tokenizer_json = json.load(f)\n",
    "\n",
    "    with open(\"reference-tokenizer/tokenizer.json\") as f:\n",
    "        reference_tokenizer_json = json.load(f)\n",
    "    \n",
    "    # Add the reference tokenizer's config to the new tokenizer's config\n",
    "    new_llama_tokenizer_json[\"normalizer\"] = reference_tokenizer_json[\"normalizer\"]\n",
    "    new_llama_tokenizer_json[\"pre_tokenizer\"] = reference_tokenizer_json[\"pre_tokenizer\"]\n",
    "    new_llama_tokenizer_json[\"post_processor\"] = reference_tokenizer_json[\"post_processor\"]\n",
    "    new_llama_tokenizer_json[\"decoder\"] = reference_tokenizer_json[\"decoder\"]\n",
    "    new_llama_tokenizer_json[\"model\"]['fuse_unk'] = reference_tokenizer_json[\"model\"]['fuse_unk']\n",
    "    new_llama_tokenizer_json[\"model\"]['byte_fallback'] = reference_tokenizer_json[\"model\"]['byte_fallback']\n",
    "\n",
    "    # Dump the new tokenizer's config\n",
    "    with open(\"new-sentencepiece-tokenizer.json\", \"w\") as f:\n",
    "        json.dump(new_llama_tokenizer_json, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Load the new tokenizer as a LlamaTokenizerFast\n",
    "    new_llama_tokenizer = LlamaTokenizerFast(\n",
    "        tokenizer_file=\"new-sentencepiece-tokenizer.json\",\n",
    "        unk_token=\"<unk>\",\n",
    "        unk_token_id=0,\n",
    "        bos_token=\"<s>\",\n",
    "        bos_token_id=1,\n",
    "        eos_token=\"</s>\",\n",
    "        eos_token_id=2,\n",
    "        pad_token=\"<pad>\",\n",
    "        pad_token_id=3,\n",
    "        padding_side=\"right\",\n",
    "    )\n",
    "\n",
    "    # Save the new tokenizer\n",
    "    new_llama_tokenizer.save_pretrained(\"new-llama-tokenizer\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train a new Llama tokenizer\")\n",
    "    parser.add_argument(\n",
    "        \"--dataset_name\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"The name of the dataset to be tokenized\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataset_split\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"The split of the dataset to be tokenized\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--hub_token\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"The token to access the dataset on the hub\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--reference_tokenizer\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"The name of the reference tokenizer to use\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_samples\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Number of samples to use from the dataset\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--vocab_size\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Vocabulary size to use for the tokenizer\",\n",
    "    )\n",
    "\n",
    "\n",
    "# How to run:\n",
    "# python train_sentencepiece.py --dataset_name \"NeelNanda/pile-10k\" --dataset_split \"train\" --hub_token \"hf_...\" --reference_tokenizer \"meta-llama/Llama-2-7b-hf\" --num_samples 2000000 --vocab_size 32000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f83cae0b-4f05-4423-b0d8-ade5cd095d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shlex\n",
    "argString = '--dataset_name \"NeelNanda/pile-10k\" --dataset_split \"train\" --hub_token \"\" --reference_tokenizer \"google/gemma-7b\" --num_samples 10000 --vocab_size 32000'\n",
    "args = parser.parse_args(shlex.split(argString))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b732c964-b702-45ba-ae11-9a396a373d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871a5688714a41d6a672742234d90787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ba965eef354f3bb217127e8d647b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e9f56d6f2b44ad688cfe70e65865c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/555 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395bf2a5-0dce-4f36-9436-3c5041707576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf18d60-ce20-4a50-ae8f-424f00a109e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
